Got it âœ… You want your Ollama model running on your Linux machine to be accessible from **anywhere in the world** (not just localhost) and usable in tools like **Continuity or Cline** for unlimited chatting.
Hereâ€™s the full run-down step by step:

---

## 1. Make Ollama Accessible on Your Server

By default, Ollama only listens on `localhost`. You need to tell it to bind to **all interfaces (0.0.0.0)**.

Edit or create the systemd override file:

```bash
sudo systemctl edit ollama.service
```

Add:

```ini
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
```

Then restart:

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

Now Ollama will listen on `http://0.0.0.0:11434`.

---

## 2. Open Firewall & Network Access

Make sure port `11434` is open:

```bash
sudo ufw allow 11434/tcp
```

Check with:

```bash
ss -tulpn | grep 11434
```

---

## 3. Expose to the Internet

You need a stable way to reach your machine:

### Option A â€“ Public IP

* If your ISP gives you a public IP, you can just use:

  ```
  http://<your-public-ip>:11434
  ```
* But IPs may change and ISPs often block ports.

### Option B â€“ DuckDNS (Free Dynamic DNS)

1. Sign up at [DuckDNS](https://www.duckdns.org/).
2. Get a subdomain like `myollama.duckdns.org`.
3. Install the DuckDNS updater on your Linux machine:

   ```bash
   mkdir ~/duckdns
   cd ~/duckdns
   echo "url=https://www.duckdns.org/update?domains=myollama&token=YOURTOKEN&ip=" > duck.sh
   chmod 700 duck.sh
   crontab -e
   ```

   Add:

   ```
   */5 * * * * ~/duckdns/duck.sh >/dev/null 2>&1
   ```

   â†’ This keeps your domain synced with your changing IP.

Now you can reach:

```
http://myollama.duckdns.org:11434
```

---

## 4. Secure the Connection (Recommended)

Running Ollama open to the internet without auth = anyone can use your GPU/CPU.
Safer way: put it behind a reverse proxy with HTTPS & auth.

### Nginx Reverse Proxy Example:

```nginx
server {
    listen 80;
    server_name myollama.duckdns.org;

    location / {
        proxy_pass http://127.0.0.1:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

Then add Letâ€™s Encrypt for HTTPS:

```bash
sudo certbot --nginx -d myollama.duckdns.org
```

(Optional) Add **basic auth** so not everyone can use it.

---

## 5. Use in Continuity / Cline

Both tools support custom Ollama endpoints.
Just point them to your domain:

```
http://myollama.duckdns.org:11434
```

Now you can chat with your model from anywhere in the world, unlimited.

---

ðŸ”¥ At this point, youâ€™ll have:

* Ollama running globally
* Domain via DuckDNS
* Optional HTTPS + password protection
* Works with Continuity or Cline

---

Do you want me to write the **secure setup with Nginx + auth** for you (so random people canâ€™t hijack your model), or do you just want it open and fast?
